# 03. 并发理论(JMM)

## JMM
JMM 全称：Java Memory Model。是一种虚拟机规范，用于屏蔽掉各种硬件和操作系统的内存访问差异，以实现让 Java 程序在各种平台下都能达到一致的并发效果。主要规定了以下两点
>1. 规定了一个线程如何以及何时可以看到其他线程修改过后的共享变量的值，即线程之间共享变量的可见性
>2. 如何在需要的时候对共享变量进行同步

而在并发编程中，我们所要处理的两个关键问题就是这两条标准的体现：线程之间如何通信以及线程之间如何同步。通信是指线程之间以何种机制来交换信息。在命令式的编程中，线程之间的通信机制有两种：共享内存和消息传递。

在共享内存并发的模型里，线程之间共享程序的公共状态，线程之间通过读-写内存中的公共状态来隐式进行通信。

在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显示进行通信，在java中典型的消息传递方式就是 wait() 和 notify()。

Java 的并发采用的就是 **共享内存模型**，Java 线程之间的通信总是隐式进行的，整个通信过程对程序员是完全透明的。这里提到的共享内存模型指的就是 Java 内存模型(简称 JMM )


## JMM 抽象结构模型  
JMM 定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。本地内存是JMM的一个抽象概念，并不真实存在。它涵盖了缓存，写缓冲区，寄存器以及其他的硬件和编译器优化。
![Alt 'JMM 内存结构抽象结构示意图'](https://s2.ax1x.com/2019/12/29/lK1Ev9.png)

从上图来看，线程A与线程B之间如要通信的话，必须要经历下面2个步骤：
>1. 线程A把本地内存A中更新过的共享变量刷新到主内存中去
>2. 线程B到主内存中去读取线程A之前已更新过的共享变量

## JVM 对 Java 内存模型的实现

![Alt 'JVM 区域图'](https://s2.ax1x.com/2019/12/30/lM9cdI.png)


JVM 在执行 Java 程序的过程中会把它所管理的内存划分为若干不同的数据区域，这些区域都有各自的用途以及创建和销毁的时间。

主要包含 2 类：
>1. 线程共享区域：方法区（Method Area）和 堆（Heap）
>2. 线程私有区域：虚拟机栈（VM Stack），本地方法栈（Native Method Stack），程序计数器（PC Register）

**具体的区分如下：**

![Alt 'JVM 区域划分'](https://s2.ax1x.com/2019/12/30/lM9RFP.jpg)

**虚拟机栈** 每一个运行在Java虚拟机上的线程都拥有自己的线程栈。每个方法在执行的时候都会创建一个栈帧用于存储局部变量表、操作数栈、动态链表、方法出口信息等。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。栈的生命周期与线程相同。

**本地方法栈** 本地方法栈与虚拟机栈的作用相似，不同之处在于虚拟机栈为虚拟机执行的Java方法服务，而本地方法栈则为虚拟机使用到的Native方法服务

**程序计数器** 程序计数器保存着每一条线程下一次执行指令位置

**堆** 用来保存程序中所创建的所有对象、数组元素

**方法区** 方法区是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据

### 数据存储总结
>1. 一个本地变量是原始类型, 它会被完全存储到栈区
>2. 一个本地变量是引用类型, 这个本地引用会被存储到栈中，但是对象本身仍然存储在堆区
>3. 一个对象的成员方法，方法中包含局部变量，存储在栈区
>4. 一个对象的成员变量，不管它是原始类型还是包装类型，都存储到堆区
>5. Static 类型的变量以及类本身相关信息都会随着类本身存储在堆区


## JMM 带来的问题

### 可见性问题
![Alt '可见性问题'](https://s2.ax1x.com/2019/12/30/lQPuQO.png)
如上图，3个count，2个为副本。启动2个线程对count进行累加，线程1更改count的时候，如果不刷新到主内存，线程2还在对原来的值进行累加。

在多线程的环境下，如果某个线程首次读取共享变量，则首先到主内存中获取该变量，然后存入工作内存中，以后只需要在工作内存中读取该变量即可。同样如果对该变量执行了修改的操作，则先将新值写入工作内存中，然后再刷新至主内存中。但是刷新时间虽然很短但并不确定。

### 竞争问题
![Alt '竞争问题'](https://s2.ax1x.com/2019/12/30/lQPnSK.png)
如果这两个加1操作是串行的，最终主内存中的count的是3。然而图中两个加1操作是并行的，当它们值更新到工作内存的副本后，会争相刷新主内存。在这里，不管是线程1还是线程2先刷新计算结果到主内存，最终主内存中的值只能是2。



## 重排序

除了共享内存和工作内存带来的问题，还存在重排序的问题：在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。

### 分类
一个好的内存模型实际上会放松对处理器和编译器规则的束缚，也就是说软件技术和硬件技术都为同一个目标而进行奋斗：在不改变程序执行结果的前提下，尽可能提高并行度。JMM对底层尽量减少约束，使其能够发挥自身优势。因此，在执行程序时，
**为了提高性能，编译器和处理器常常会对指令进行重排序**。一般重排序可以分为如下三种：
![Alt "指令重排序"](https://s2.ax1x.com/2019/12/30/lMlgJS.png)

>1. 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序；
>2. 指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果**不存在数据依赖性**，处理器可以改变语句对应机器指令的执行顺序；
>3. 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行的。

注: 指令并行重排序和内存系统重排序统称为处理器排序

### 依赖性
我们知道重排序是编译器或者系统的优化。但是如果有些指令存在依赖性的话，进行重排序会导致错误。

* 数据依赖性  
如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。数据依赖分为下列3种类型，这3种情况，只要重排序两个操作的执行顺序，程序的执行结果就会被改变。

|名称 | 说明 | 示例|
| :-: | :-: | :-: |
|写后读 | 写一个变量，再读这个变量  | a = 1; b = a; |
|写后写| 写一个变量，再写这个变量 | a = 1; a = 2; |
|读后写| 读一个变量, 再写这个变量 | a = b; b = 1; |

这三种操作都存在数据依赖性，如果重排序最终会导致结果受到影响。  

* 控制依赖性
```java
public void method() {
    
    if (flag) {
        int num = a * a;
        return num;
    }
}
```
在上面的代码中，我们可以知道变量 num 的值依赖于 if( flag) 的判断值, 这里就叫控制依赖。

控制依赖在单线程的情况下，对存在控制依赖的操作重排序，不会改变执行结果。但是在多线程的情况下，就有可能出现问题。

```java
public class Demo {
    int a = 0;
    boolean flag = false;
    
    public void init() {
        // 1
        a = 1;
        // 2
        flag = true;
    }
    
    public void use() {
        // 3
        if (flag) {
            // 4
            int i = a * a;
        }
    }
}
```

在程序中，当代码中存在控制依赖性时，会影响指令序列执行的并行度。为此，编译器和处理器会采用猜测（Speculation）执行来克服控制相关性对并行度的影响。以处理器的猜测执行为例，执行线程B的处理器可以提前读取并计算a*a，然后把计算结果临时保存到一个名为重排序缓冲（Reorder Buffer，ROB）的硬件缓存中。当操作3的条件判断为真时，就把该计算结果写入变量i中。猜测执行实质上对操作3和4做了重排序，问题在于这时候，a的值还没被线程A赋值。

当操作1和操作2重排序，操作3和操作4重排序时，可能会产生什么效果？操作1和操作2做了重排序。程序执行时，线程A首先写标记变量flag，随后线程B读这个变量。由于条件判断为真，线程B将读取变量a。此时，变量a还没有被线程A写入，这时就会发生错误！

所以在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。


### as-if-serial
不管如何重排序，都必须保证代码在单线程下的运行正确，连单线程下都无法正确，更不用讨论多线程并发的情况，所以就提出了一个 as-if-serial 的概念。

**as-if-serial:** 不管怎么重排序 (编译器和处理器为了提高并行度)，(单线程)程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守 as-if-serial 语义。  
为了遵守 as-if-serial 语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作依然可能被编译器和处理器重排序。

**as-if-serial** 语义把单线程程序保护了起来，遵守 as-if-serial 语义的编译器、runtime 和处理器可以让我们感觉到：单线程程序看起来是按程序的顺序来执行的。asif-serial 语义使单线程程序员无需担心重排序会干扰他们，也无需担心内存可见性问题。

### 内存屏障
Java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序，从而让程序按我们预想的流程去执行
>1. 保证特定操作的执行顺序
>2. 影响某些数据（或则是某条指令的执行结果）的内存可见性

编译器和 CPU 能够重排序指令，保证最终相同的结果前提，尝试优化性能。插入一条 Memory Barrier 会告诉编译器和 CPU：不管什么指令都不能和这条 Memory Barrier 指令重排序。

Memory Barrier 所做的另外一件事是强制刷出各种 CPU cache，如一个 Write-Barrier（写入屏障）将刷出所有在 Barrier 之前写入 cache 的数据，因此，任何 CPU 上的线程都能读取到这些数据的最新版本。

JMM把内存屏障指令分为4类：
| 屏障类型| 说明 | 实例 |
| :-: | :-: | :-: |
| LoadLoadBarriers| Load1; LoadLoadBarriers; Load2 | 确保 Load1 数据的读取 在 load2 及 后续指令的读取之前|
| StoreStoreBarriers | Store1; StoreStoreBarriers; Store2; | 确保 Store1 数据的写入(刷新到主内存) 在 Store2 及后续指令的读取之前 |
|LoadStoreBarriers| Load1; LoadStoreBarriers; Store2; | 确保 Load1 数据的读取 在Store2 及后续指令的写入之前 |
| StoreLoadBarriers | Store1; StoreLoadBarriers; Load2; | 确保 Store2 数据的写入 在 Load2 及后续指令的读取之前 |

StoreLoad Barriers是一个“全能型”的屏障，它同时具有其他3个屏障的效果。现代的多处理器大多支持该屏障（其他类型的屏障不一定被所有处理器支持）。



## Happens-Before

在 Java 规范提案中为让大家理解内存可见性的这个概念，提出了 Happens-Before 的概念来阐述操作之间的内存可见性。 

JMM 这么做的原因是：程序员对于这两个操作是否真的被重排序并不关心，程序员关心的是程序执行时的语义不能被改变（即执行结果不能被改变）。因此，Happens-Before 关系本质上和 as-if-serial 语义是一回事。as-if-serial 语义保证单线程内程序的执行结果不被改变，Happens-Before 关系保证正确同步的多线程程序的执行结果不被改变。

### 定义
>1. 如果一个操作 Happens-Before 另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。

>2. 两个操作之间存在 Happens-Before 关系，并不意味着Java平台的具体实现必须要按照 Happens-Before 关系指定的顺序来执行。如果重排序之后的执行结果，与按 Happens-Before 关系来执行的结果一致，那么这种重排序并不非法（也就是说，JMM允许这种重排序）。

上面的定义看起来很矛盾，其实它是站在不同的角度来说的。  
1: 站在 Java 程序员的角度来说：JMM保证，如果一个操作 Happens-Before 另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。  

2: 站在编译器和处理器的角度来说：JMM 允许，两个操作之间存在 Happens-Before 关系，不要求 Java 平台的具体实现必须要按照 Happens-Before 关系指定的顺序来执行。如果重排序之后的执行结果，与按 Happens-Before 关系来执行的结果一致，那么这种重排序是允许的

### as-if-serial VS Happens-Before
>1. as-if-serial 语义保证单线程内程序的执行结果不被改变，Happens-Before 关系保证正确同步的多线程程序的执行结果不被改变。
>2. as-if-serial 语义给编写单线程程序的程序员创造了一个幻境：单线程程序是按程序的顺序来执行的。Happens-Before 关系给编写正确同步的多线程程序的程序员创造了一个幻境：正确同步的多线程程序是按 Happens-Before 指定的顺序来执行的。
>3. as-if-serial语义和 Happens-Before 这么做的目的，都是为了在不改变程序执行结果的前提下，尽可能地提高程序执行的并行度。

### 具体的规则
>1. 程序顺序规则（Program Order Rule）: 一个线程中的每个操作，Happens-Before 于该线程中的任意后续操作。

个人理解：
```java
// 情况一：
int a = 1;
int b = a + 1;

// 情况二:
int c = 1;
int d = 2;
int e = c + d;
```
在同一个线程内：
情况一：同一个线程中的操作存在依赖性(b 的值依赖于 a)，那么前面的动作的操作结果对于后面的动作可见。

情况二：同一个线程的操作不存在依赖性（c, d 直接没有什么依赖关系）, 允许重排序，只要这种重排序不违反 **程序顺序规则**。( e 依赖于 c 和 d 的值，所以 c 和 d 之间可以重排序，但是不能跨 e 排序，e 的结果依赖于 c, d)

也就是：在同一个线程里面：编译器未必一定会按照这个代码的顺序进行编译，但是编译器保证：最终的结果等于顺序执行的结果 

>2. 监视器锁规则（Monitor Lock Rule）：同一个锁的unlock操作 Happens-Before 此锁的lock操作

个人理解：  
unlock 和 lock 都是针对同一个锁实例。线程 A 释放了锁 lock, 接着线程 B 锁定了 lock。那么线程 A 解锁和解锁前的操作对线程 B 可见(线程 A 和 B 可以为同一个线程)。

>3. volatile变量规则（volatile Variable Rule）：对一个 volatile 变量的 写操作happens-before 后续（ Subsequent）每一个针对该变量的读操作。

个人理解：
线程 A 对 volatile 修饰的变量 v 进行操作，线程 B(可以是同一个线程，也就是线程A) 对变量 v 进行读取，那么线程 A 对 变量 v 多的操作的结果，对线程 B 是可见的

>4. 线程启动规则（Thread Start Rule）: Thread对象的start()方法 和 书写在 start 方法前面的代码 Happens-Before 此线程的每一个动作。

个人理解：  
线程 T1 执行了一段业务代码后，调用了线程 T2 的 start 方法。那么 T1 执行的业务代码的结果对于线程 T2 可见。

>5. 线程终止规则（Thread Termination Rule）: 线程中的任何操作都 Happens-Before 其它线程检测到该线程已经结束。

个人理解：
一个线程结束了, 在他结束的时，他的所有操作结果对其他线程可见。例如：2 个线程 A, B。在线程 A 调用 B.join(), 这时 A 会被挂起，等待线程 B 执行完成才恢复。当 B.join() 返回时，A 指定 B 结束了。那么在 B 执行的这段时间所有的操作结果，对 A 可见。

>6. 线程中断规则（Thread Interruption Rule）: 一个线程在另一个线程上调用 interrupt, Happens-Before 被中断线程检测到 interrupt 被调用。

个人理解：
线程 A 做了一些操作，然后调用线程 B 的 interrupt, 那么线程 A 做的操作结果对线程 B 可见。

>7. 对象终结规则（Finalizer Rule）: 一个对象的初始化完成（构造函数执行结束）Happens-Before它的finalize()方法的开始

>8. 传递性（Transitivity）: 如果操作A Happens-Before B，B Happens-Before C，那么可以得出操作A Happens-Before C

### Happens-Before规则的真正意义

我们已经知道，导致多线程间可见性问题的两个“罪魁祸首”是 CPU 缓存和重排序。那么如果要保证多个线程间共享的变量对每个线程都及时可见，一种极端的做法就是禁止使用所有的重排序和 CPU 缓存。即关闭所有的编译器、操作系统和处理器的优化，所有指令顺序全部按照程序代码书写的顺序执行。去掉 CPU 高速缓存，让 CPU 的每次读写操作都直接与主存交互。
当然，上面的这种极端方案是绝对不可取的，因为这会极大影响处理器的计算性能，并且对于那些非多线程共享的变量是不公平的。
重排序和 CPU 高速缓存有利于计算机性能的提高，但却对多 CPU 处理的一致性带来了影响。为了解决这个矛盾，我们可以采取一种折中的办法。我们用分割线把整个程序划分成几个程序块，在每个程序块内部的指令是可以重排序的，但是分割线上的指令与程序块的其它指令之间是不可以重排序的。在一个程序块内部，CPU 不用每次都与主内存进行交互，只需要在 CPU 缓存中执行读写操作即可，但是当程序执行到分割线处，CPU 必须将执行结果同步到主内存或从主内存读取最新的变量值。那么，Happens-Before 规则就是定义了这些程序块的分割线。下图展示了一个使用锁定原则作为分割线的例子：
![Alt 'Happens-Beofore'](https://s2.ax1x.com/2020/01/02/ltWA2j.jpg)
如图所示，这里的 unlock M 和 lock M 就是划分程序的分割线。在这里，红色区域和绿色区域的代码内部是可以进行重排序的，但是 unlock 和 lock 操作是不能与它们进行重排序的。即第一个图中的红色部分必须要在 unlock M 指令之前全部执行完，第二个图中的绿色部分必须全部在 lock M 指令之后执行。并且在第一个图中的 unlock M 指令处，红色部分的执行结果要全部刷新到主存中，在第二个图中的 lock M 指令处，绿色部分用到的变量都要从主存中重新读取。
在程序中加入分割线将其划分成多个程序块，虽然在程序块内部代码仍然可能被重排序，但是保证了程序代码在宏观上是有序的。并且可以确保在分割线处，CPU 一定会和主内存进行交互。Happens-Before 原则就是定义了程序中什么样的代码可以作为分隔线。并且无论是哪条 Happens-Before 原则，它们所产生分割线的作用都是相同的。


## JMM 的设计

站在JMM设计者的角度，在设计JMM时需要考虑两个关键因素:
>1. **程序员对内存模型的使用**  

程序员希望内存模型易于理解、易于编程。程序员希望基于一个强内存模型来编写代码。

>2. **编译器和处理器对内存模型的实现**  

编译器和处理器希望内存模型对它们的束缚越少越好，这样它们就可以做尽可能多的优化来提高性能。编译器和处理器希望实现一个弱内存模型。

另外还有一个事：Happens-Before 关系的处理的重排序可以分为两类
>1. 会改变程序执行结果的重排序
>2. 不会改变程序执行结果的重排序


JMM对这两种不同性质的重排序，采用了不同的策略
>1. 对于会改变程序执行结果的重排序，JMM 要求编译器和处理器必须禁止这种重排序
>2. 对于不会改变程序执行结果的重排序，JMM 对编译器和处理器不做要求

JMM 的设计图为：
![Alt 'JMM设计示意图'](https://s2.ax1x.com/2020/01/02/ltISN6.png)

从图可知道：
>1. JMM 向程序员提供的 Happens-Before 规则能满足程序员的需求。JMM 的 Happens-Before 规则不但简单易懂，而且也向程序员提供了足够强的内存可见性保证。

>2. JMM 对编译器和处理器的束缚已经尽可能少。从上面的分析可以看出，JMM 其实是在遵循一个基本原则：只要不改变程序的执行结果（指的是单线程程序和正确同步的多线程程序），编译器和处理器怎么优化都行。例如，如果编译器经过细致的分析后，认定一个锁只会被单个线程访问，那么这个锁可以被消除。再如，如果编译器经过细致的分析后，认定一个 volatile 变量只会被单个线程访问，那么编译器可以把这个 volatile 变量当作一个普通变量来对待。这些优化既不会改变程序的执行结果，又能提高程序的执行效率。


参考  
《Java 多线程编程实践实战指南（核心篇）》  
[再有人问你Java内存模型是什么，就把这篇文章发给他。](https://www.hollischuang.com/archives/2550)  
[Java 内存模型详解](https://juejin.im/post/5d3eafe95188255d3d296e09)  
[java内存模型以及happens-bofore原则](https://www.codercc.com/post/812b4c63.html)  
[JMM和底层实现原理](https://www.jianshu.com/p/8a58d8335270)  
[大厂很可能会问到的JMM底层实现原理](https://blog.csdn.net/weixin_44367006/article/details/99656558)  
[从Java多线程可见性谈Happens-Before原则](https://segmentfault.com/a/1190000011458941)